\documentclass[12pt]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amssymb,amsmath,amsthm, mathtools}
\usepackage{subfigure}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\DeclareMathOperator{\trace}{trace}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}


\newtheorem{note}{Замечание}
\newtheorem{theorem}{Теорема}
\newtheorem{dfn}{Определение}
\newtheorem{utv}{Утверждение}
\newtheorem{example}{Пример}

\begin{document}
\subsection{Основные определения и понятия}
Пусть $A = \{ 1, 2, \ldots, a\}$ и $B = \{1, 2,\ldots b\}$, $a, b < \infty$ --- два конечных
множества. Пусть $S = \{1, 2,  \ldots,  N\}$ --- конечное множество индексов.
Пусть $X = \{X_i \mid i \in S\}$ — многомерная случайная величина, такая что каждая компонента $X_j$ , являющаяся одномерной случайной величиной, принимает значение $x_j$ и определена в своем вероятностном пространстве. Считается, что $\forall j \ X_j$ дискретны, определены на одном
вероятностном пространстве и множество значений — конечно.
Для удобства рассмотрения можно представлять, что множество
индексов $S$ задает множество точек на плоскости. Соответственно, рассматривается реализация многомерной случайной величины $X$ в этих точках. Введенная таким образом случайная величина $X$ называется случайным полем.

Конкретная реализация $x = (x_1, \ldots, x_N)$ многомерной случайной величины $X$, то есть совместное событие $(X_1 = x_1, \ldots, X_N = x_N)$ (кратко $X = x$), называется конфигурацией $X$. Пусть X --- случайное поле со значениями на множестве A, то есть $\forall i  \ X_i \in A$. Если $x$ --- какая-то конкретная конфигурация $X$, то $\mathcal{X}$ ---
множество всех возможных конфигураций:
\begin{gather*}
\mathcal{X} = \{ x = (x_1, \ldots, x_N \mid x_i \in A \forall i \in S\}.
\end{gather*}
Система соседства --- это множество $\partial = \{\partial i\mid i \in S \}$, где $\partial i$ --- множество элементов из $S $, называемое шаблоном соседства для элемента $i$, такое что:
\begin{gather*}
\begin{cases}
i \not \in \partial i, \\
i \in \partial j \Leftrightarrow j \in \partial i
\end{cases}
\end{gather*}
Например, на рис. 1 (a) система соседства $\partial = \{ \{i-1, i+1 \} \mid i \in S\}$
\begin{figure}[h!]
\centering
\subfigure[Случайное марковское поле]{
\includegraphics[width=.8\linewidth]{./figures/mrf.pdf}
}
\subfigure[Скрытое случайное марковское поле]{
\includegraphics[width=.8\linewidth]{./figures/hmrf.pdf}
}\label{fig:mrf}
\caption{Примеры}
\end{figure}
\begin{dfn}
Случайное поле $X$ называется марковским случайным полем в соответствии с системой соседства $\partial$ тогда
и только тогда, когда $\forall i$:

\begin{gather*}
\begin{cases}
P(X = x) > 0 \forall x \in \mathcal{X}, \\
p(X_i = x_i \mid X_j = x_j, j \in S \setminus \{i\}) = P(X_i = x_i \mid X_j = x_j, j \in \partial i),
\end{cases}
\end{gather*}
\end{dfn}

\begin{dfn}
Клика $c$ для системы соседства $\partial$ --- множество элементов из $S$,  такое что $\forall s, r \in c, s \ne r \Rightarrow r \in \partial s$. Несложно заметить,
что любое подмножество клики --- также клика (считается, что любое одноэлементное множество также является кликой; в этом случае такая клика называется тривиальной). Например, кликами, содержащими элемент $i$ (рис. \ref{fig:mrf} (a)), будут являться множества $\{i, i+1 \}, \{i-1, i\}$ и тривиальная клика $i$.
\end{dfn}

Пусть $c$ --- клика, а $x_c$ --- ограничение конфигурации $x$ на $c$, то есть $x_c = (x_{i_1}, x_{i_2}, \ldots x_{i_{|c|}})$, где $i_j \in c, j = 1, \ldots, |c|$. Пусть $C(\partial)$ --- множество всех клик для системы соседства $\partial$, тогда потенциальная функция $V_c(x_c)$ определяется как любая функция $V : C(\partial) \rightarrow R$.

\begin{dfn}
Дискретное распределение называется распределени-
ем Гиббса, если
\begin{gather}
P(X=x) = \frac1Z \exp \left( - \sum\limits_{x\in C(\partial)} V_c(x_c) \right), 
\label{eqn:distr}
\end{gather}
где $Z$ --- нормирующая константа, такая что:
\begin{gather}
Z = \sum\limits_{x\in \mathcal{X}} \exp \left( - \sum\limits_{x\in C(\partial)} V_c(x_c)  \right).
\label{eqn:const}
\end{gather}
\end{dfn}
Очевидно, что в случае нулевых потенциальных функций для какого--нибудь набора клик, соответствующие члены будут просто отсутствовать в формулах \eqref{eqn:distr}, \eqref{eqn:const}.
Наиболее важной теоремой, связывающей марковские случайные
поля и распределение Гиббса, является следующая
\begin{theorem}[Hammersley-Clifford]
$X$ --- марковское случайное поле тогда и только тогда, когда $P(X = x)$--- распределение Гиббса.
\end{theorem}
Таким образом, имеется возможность вычислять вероятность кон-
фигурации для любого марковского случайного поля по формулам \eqref{eqn:distr}, \eqref{eqn:const}.

\begin{dfn}
Скрытое марковское случайное поле --- пара случайный полей $(X, Y)$, такая что 
\begin{enumerate}
\item $X = \{ X_i \mid i \in S \}$ --- так называемое <<скрытое>> (или, другими словами, ненаблюдаемое) марковское поле со значениями в $A$.
\item  $Y = \{ Y_i \mid i \in S \}$  наблюдаемое (вовсе не обязательно марковское) случайное поле со значениями в $B$. Важно, что $\forall j \in S, \forall d \in A$,известны условные распределения $P(Y_i \mid X_i = d)$.
\item Для любой конфигурации $x\in \mathcal{X}$ случайные величины $Y_i$ условно независимы, то есть
\begin{gather*}
P(Y\mid X = x) = \prod\limits_{i \in S} P(Y_i\mid X_i = x_i).
\end{gather*}
\end{enumerate}
\end{dfn}
Скрытое марковское случайное поле можно себе представлять как
наблюдаемое случайное поле $Y$, каждой точке которого соответствует ненаблюдаемое (скрытое) марковское случайное поле $X$. Например, скрытое марковское случайное поле, соответствующее марковскому
случайному полю на рис. \ref{fig:mrf} (a), изображено на рис. \ref{fig:mrf} (b)

Очевидно, что 
\begin{gather*}
P(Y = y, X = x) = P(Y = y\mid X = x)P(X = x) = \\
= P(X = x)\prod\limits_{i \in S} P(Y_i = y_i\mid X_i = x_i).
\end{gather*}
Совместное распределение $(X_i, Y_i)$ с шаблоном соседства $\partial$ определяется как
\begin{gather*}
P(X_i = x_i, Y_i = y_i \mid X_j = x_j, j \in \partial i) = \\
= P(Y_i = y_i\mid X_i = x_i) P(X_i = x_i \mid X_j = x_j, j \in \partial i),
\end{gather*}
а распределение $Y_i$ с шаблоном соседства $\partial i$ определяется как
\begin{gather*}
P(Y_i = y_i \mid X_j = x_j, j \in \partial i) = \\
=\sum\limits_{d\in A} P(Y_i = y_i X_j = d \mid X_j = x_j, j \in \partial i) = \\
= \sum\limits_{d\in A} P(Y_i = y_i \mid X_j = d) P(X_i = d\mid X_j = x_j, j \in \partial i)
\end{gather*}

\begin{example}
Данный пример, представленный на рис. \ref{fig:hmrf:ex} представляет собой пример отчистки изображения от шума. Каждый $X_i, Y_i$ имеет распределение на множестве $\{-1, +1\}$.
Совместное распределения для такого случайного марковского поля будет иметь вид:
\begin{gather*}
p(x, y) = -\frac{1}{Z} \exp\{ -E(x, y)\}, \\ 
E(x, y) = \alpha \sum\limits_i x_i - \beta \sum\limits_{ \{i, j \} } x_i x_j -   \\ \gamma \sum\limits_{ \{i, j \} } x_i y_j
\end{gather*}
\begin{figure}[hhh]
\centering
\includegraphics[width=.5\linewidth]{./figures/image.pdf}
\label{fig:hmrf:ex}
\caption{Пример скрытого марковского поля}
\end{figure}
\end{example}

\newpage
\subsection{Скрытые марковские модели}
\begin{dfn}
Скрытая марковская модель --- это дважды случайный процесс, то есть модель, состоящая из $N$ состояний,
в каждом из которых некоторая система может принимать одно из
$M$ значений какого-либо параметра. Вероятности переходов между
состояниями задаются матрицей вероятностей $A = (a_{ij})$, где $a_{ij}$ — вероятность перехода из $i$-го в $j$-е состояние. Вероятности выпадения каждого из $M$ значений параметра в каждом из $N$ состояний
задаются матрицей $B = (b_{jk})$, где $b_{jk}$ — вероятность выпадения $k$-го
значения параметра в $j$-м состоянии. Вероятность наступления начального состояния задается вектором $\pi = (\pi_i)$ , где $\pi_i$ --- вероятность
того, что в начальный момент система окажется в $i$-м состоянии.
\end{dfn}

Полагается, что наблюдаемые значения $Y_k$ зависят только от соответствующего им скрытого состояния марковской цепи $X_j$, то есть $P(Y = y \mid X = x) = \prod\limits_{i = 0}^{n} P(Y_i = y_i \mid X_i = x_i)$. Таким образом, скрытой
марковской моделью называется тройка $\lambda = (A, B, \pi)$.

Возьмем скрытую марковскую модель, функционирующую в дискретном времени. Пусть при некотором ее функционировании при проходе через состояния $x_0, \ldots, x_n$ на выход подавались буквы $y_0, \ldots, y_n$ соответственно. Рассмотрим многомерную случайную
величину
\begin{gather*}
Z = (Z_1, \ldots, Z_{2n + 2}) = (X_0, Y_0, \ldots, X_n, Y_n),
\end{gather*}
причем вероятность $P(X_i = x_i \mid X_{i-1} = x_{i-1})$ --- это вероятность перехода $a(x_{i-1}, x_{i})$ из состояния $x_{i-1}$ в состояние $x_i$, а вероятность $P(Y_i  = y_i \mid X_i = x_i)$ это вероятность $b(y_i, x_i)$ выдачи буквы $y_i$ в состоянии $x_i$. Назовем такую многомерную случайную величину скрытой порожденной случайной величиной.
\begin{figure}[h!]
\includegraphics[width=.8\linewidth]{./figures/hmm.jpg}
\label{fig:hmm}
\caption{Скрытая марковская модель}
\end{figure}
\begin{utv}
Любая скрытая порожденная случайная величи-
на — это случайное марковское поле специального вида (см. рис. \ref{fig:hmm}).
\end{utv}
\subsection{Алгоритм Витерби}
На примере скрытой марковской цепи опишем алгоритм Витерби (существует его обобщение на скрытые марковские поля).
Данный алгоритм вычисляет наиболее вероятную последовательность скрытых состояний $X_i$, при условии наблюдаемой последовательности состояний $Y_i$. 

Наиболее вероятная последовательность состояний $x_1, \ldots, x_n$ будет вычисляться при помощи следующих рекуррентных соотношений
\begin{itemize}
\item $\phi_{1,k} = b_{1,k} \cdot \pi_k$
\item $\phi_{\ell,k} = b_{\ell,k} \cdot \max\limits_{i \in S} (a_{i, k} \cdot \phi_{\ell-1, i})$.
\end{itemize}
Здесь $\phi_{\ell, k}$ — это вероятность наиболее вероятной последовательности состояний, ответственная за появление первых $\ell$ наблюдаемых символов, завершающаяся в состоянии $k$.
Иными словами, если мы наблюдаем состояние $y_1$, то вероятность делится на следующие вероятности:
\begin{enumerate}
\item Получить скрытое состяние <<1>> с вероятностью $\pi_1$ и получить наблюдаемое состояние $y_1$ с вероятностью $b_{1,1}$.
\item Получить скрытое состяние <<2>> с вероятностью $\pi_2$ и получить наблюдаемое состояние $y_1$ с вероятностью $b_{2,1}$.
\item \ldots
\end{enumerate}

\end{document}