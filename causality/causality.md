[Исходная статья: Causal Discovery with Continuous Additive Noise Models](https://arxiv.org/abs/1309.6779) - *Отличается от первоначально выбранной!*

Популярным средством измерения силы причинно-следственной связи процессов, протекающих в природе или обществе, является корреляция. Однако существует принцип, который формулируется довольно просто: "correlation does not mean causation". Пренебрежение им может порождать так называемые «ложные корреляции»: например, количество гнёзд аистов влияет на рождаемость в европейских странах или количество фильмов, в которых снялся Николас Кейдж, может быть каким-то образом связано с гибелью людей в бассейнах.
![Storks](http://i.imgur.com/sKwYC0Y.png)
![Nicolas Cage](http://i.imgur.com/DvqDAbM.png)

Возникает естественный вопрос: в каких случаях "correlation DOES mean causation"? На этот вопрос пытается ответить [группа исследователей из Института интеллектуальных система Макса Планка.](https://ei.is.tuebingen.mpg.de/research_groups/causal-inference-group) Оказывается, при выполнении некоторых условий, можно восстановить причинно-следственную связь из результатов наблюдений.

Для исследования причинно-следственных связей между процессами [Джуда Пёрл](https://ru.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D0%BB,_%D0%94%D0%B6%D1%83%D0%B4%D0%B0) предложил использовать фунцкиональные причинные модели (functional causal models). По уравнениям, описывающим причинную модель, можно построить граф, который отображает зависимости между случайными величинами, но не описывает характера таких зависимостей. Таким образом, граф содержит меньше информации о модели, однако позволяет ввести в терминах теории графов некоторые критерии, упрощающие определение условной независимости между переменными. Эти понятия неразрывно связаны с понятием байесовских сетей.


### Определения

[more definitions come here...]

Ориентированный ациклический граф (DAG, directed acyclic graph) — это ориентированный граф, в котором отсутствуют направленные циклы, то есть пути, начинающиеся и закончивающиеся в одной и той же вершине.

Пусть $ (X_i)_{i \in V} $ — семейство случайных величин, $ G = (V, E) $ — DAG с вершинами $ E \subseteq V^2 $. Как правило, в литературе используют обозначения $ X_i $ и $ i $ для соответствующих вершин графа.

$ X_i $ называется вершиной-родителем вершины $ X_j $, если $ (i, j) \in E $, и вершиной-ребёнком, если $ (j, i) \in E $. Множество вершин-родителей вершины $ X_j $ обозначатся $ pa_j $, множество вершин-детей — $ ch_j $.

Направленным путём в графе $ G $ называется последовательность различных вершин $ X_{i_1}, ..., X_{i_n} $, для которых $ (i_k, i_{k+1}) \in E $ или $ (i_{k+1}, i_k) \in E $ для $ k = 1, ..., n - 1 $. Если между вершинами $ X_{i_1} $ и $ X_{i_n} $ есть направленный путь, вершина $ X_{i_n} $ называется потомком вершины $ X_{i_1} $.

Множество всех потомков вершины $ X_i $ обозначается $ de_i $, множество вершин не-потомков — $ nd_i $.

Путь между $ X_{i_1} $ и $ X_{i_n} $ называется заблокированным множеством $ S $ (не включая ни $ X_{i_1} $, ни $ X_{i_n} $), если существует такая вершина $ X_{i_k} $, что выполняется одно из условий:
1. $ X_{i_k} \in S $ и
  - $ X_{i_{k-1}} \rightarrow X_{i_k} \rightarrow X_{i_{k+1}} $ или
  - $ X_{i_{k-1}} \leftarrow X_{i_k} \leftarrow X_{i_{k+1}} $ или
  - $ X_{i_{k-1}} \leftarrow X_{i_k} \rightarrow X_{i_{k+1}} $
2. $ X_{i_{k-1}} \rightarrow X_{i_k} \leftarrow X_{i_{k+1}} $ и ни $ X_{i_k} $, ни её потомки не принадлежат $ S $.

Говорят, что два непересекающихся множества вершин $ A $ и $ B $ d-разделены (непересекающимся с ними множеством $ S $), если каждый путь между вершинами $ A $ и $ B $ заблокирован множеством $ S $.

Совместное распределение $ \mathbb{P}^{(X_i)_{i \in V}} $ называется марковым относительно DAG $ G $, если $ A $ и $ B $ d-разделены $ C $ $ \Rightarrow A ⫫ B \mid C $ для всех непересекающихся множеств $ A, B, C $.

Два графа называются марковски эквивалентными, если в них выполняются одинаковые наборы d-разделимости, т. е. имеют одинаковые наборы условных независимостей совместного распределения.

Лемма. Два DAG являются марковски эквивалентными тогда и только тогда, когда они имеют одинаковые остовы и одинаковые v-структуры.

Понятие причинной минимальности может быть сформулировано следующим способом.

Предложение. Пусть заданы случайный вектор $ X = (X_1, \dots, X_p) $ и совместное распределение на нём $ \mathcal{L} $ — марково относительно $ \mathcal{G} $. Тогда $ \mathcal{L} $ отвечает причинной минимальности относительно $ \mathcal{G} $ тогда и только тогда, когда $ \forall X_j \forall Y \in PA_{j}^{\mathcal{G}} выполняется $ X_j \not\!\perp\!\!\!\perp Y | PA_{j}^{\mathcal{G}} \ {Y} $.

Для всех рассматриваемых графов предполагается ацикличность (отсутствие циклов в причинно-следственных связях) и достаточность (в рассмотрение принимаются все переменные, являющиеся общей причиной двух и более других переменных).

Для строгого определения истинного причинного графа пользуются так называемой do-нотацией, предложенной Джудой Пёрлом.

Определение. Пусть $ X = (X_1, \dots, X_p) $ — семейство случайных величин с совместным распределением $ \mathcal{L} $, абсолютно непрерывным относительно меры Лебега. Для направленного ациклического графа $ \mathcal{G} $, заданного над $ X $, будем называть интервенционным распределением (interventional distribution) $ do(X_j = \tilde{p}(x_j)) $ величин $ X_1, \dots, X_p $
$$ p(x_1, \dots, x_p | do(X_j = \tilde{p}(x_j))) = \prod_{i \ne j}^p p(x_i | x_{PA_i} ) \cdot \tilde{p}(x_j), $$
если $ p(x_1, \dots, x_p) > 0 $, и 0 в другом случае.

Иначе говоря, запись $ do(X_j = \tilde{p}(x_j )) $ означает назначение значения переменной $ X_j $ случайным образом из распределения $ \tilde{p}(x_j) $ независимо от значения её родителей.

Под $ x_{PA_i} $ понимается набор всех $ X_j $, которые являются непосредственными родителями $ X_i $ в графе $ \mathcal{G} $.

Выражение $ p(x_1, \dots, x_p | do(X_j = \tilde{x}_j, j \in J)) $ задаёт распределение над $ X_1, \dots, X_p $.

Определение. Пусть даны $ \mathcal{L}(X) $ — распределение над $ X_1, \dots, X_p $ — и распределения $ \mathcal{L}_{do(X_j = \tilde{p}(x_j), j \in J)}(X) $ для всех $ J \subset V = {1, \dots, p} $. Будем называеть граф $ \mathcal{G}_c $ истинным причинным графом, если:
- $ \mathcal{G}_c $ — это направленный ациклический граф,
- распределение $ \mathcal{L}(X) $ марково относительно $ \mathcal{G}_c $;
- для всех $ J \subset V $ и $ \tilde{p}(x_j), j \in J $ распределение $ \mathcal{L}_{do(X_j = \tilde{p}(x_j), j \in J)}(X) $ совпадает с $ p(x_1, \dots, x_p | do(X_j = \tilde{p}(x_j)), j \in J), вычисленным для $ \mathcal{G}_c $ в предыдущем определении.

Вообще говоря, может существовать несколько истинных причинных графов.

### Математическая постановка задачи.



### Функциональные модели

Structural equation model (SEM) или иначе функциональная модель задаётся как пара $ (\mathcal{S}, \mathcal{L}(N)) $, где $ \mathcal{S} = (S_1, \dots, S_p) $ — набор $ p $ уравнений

$$ S_j: \quad X_j = f_i(PA_j, N_j), \quad j = 1, \dots, p $$

и $ \mathcal{L}(N) = \mathcal{L}(N_1, \dots, N_p) $ — совместное распределение шумовых переменных, от которых требуется независимость в совокупности.

Функциональная модель задаёт, как образом $ PA_j $ оказывают влияние на $ X_j $.

Граф, полученный из функциональной модели, позволяет факторизовать совместную плотность распределения по следующей формуле:

$$ \mathcal{L}(X) = \mathcal{L}(X_1, \dots, X_p) = \prod_j p(X_j \mid pa_j). $$

Т.\,е., достаточно рассматривать непосредственных родителей каждой из вершин.

Предложение. Пусть $ X_1, \dots, X_p $ — случайные величины, $ \mathcal{L}(X) $ — их совместное распределение относительно $ \mathcal{G} $. Тогда существуют SEM $ (\mathcal{S}, \mathcal{L}(N)) $ и граф $ \mathcal{G} $, порождающие распределение $ \mathcal{L}(X) $.

Дополнительно вводится определение идентифицируемости.

Определение. Пусть распределение $ \mathcal{L}(X) = \mathcal{L}(X_1, \dots, X_p) $ получено по (неизвестной) функциональной модели с графом $ \mathcal{G}_0 $, в частности, $ \mathcal{L}(X) $ марково относительно $ \mathcal{G}_0 $. Будем называеть $ \mathcal{G}_0 $ идентифицируемым по $ \mathcal{L}(X) $, если распределение $ \mathcal{L}(X) $ не могло быть получено по той же функциональной модели с графом $ \mathcal{G} \ne \mathcal{G}_0 $.

Как правило, $ \mathcal{G}_0 $ не идентифицируем по $ \mathcal{L}(X) $, поскольку совместное распределение $ \mathcal{L}(X) $ обычно является марковым относительно большого количества различных графов. Однако наложение некоторых дополнительных требований позволяет добиться идентифицируемости ограниченных моделей.

### Идентифицируемость непрерывных моделей с аддитивным шумом

Рассмотрим функциональные модели следующего вида: $ X_j = f_j(PA_j) + N_j, j = 1, \dots, p $, где $ N_j $ имеют строго положительные распределения.

Для таких моделей требование причинной минимальности упрощается.

Предложение. Пусть в описанной модели функции $ f_i $ не являются постоянными ни по одному из их аргументов, т.\,е. для всех $ i \in PA_j $ существуют некоторые $ x_{PA_j \ {i} $ и $ x_i \ne x_i' $ такие, что $$ f_j(x_{PA_j \ {i}}, x_i) \ne f_j(x_{PA_j \ {i}}, x_i') $$.
Тогда совместное распределение, полученное по этой модели, отвечает требованию причинной минимальности относительно соответствующего графа.

#### Модели с аддитивным шумом двух переменных

Идентифицируемость моделей с аддитивным шумом двух переменных возможна при выполнении любопытного условия.

Условие 1. Модель двух переменных с аддитивным шумом $ X_1 = N_1 $ и $ X_2 = f_2(X_1) + N_2 $ является идентифицируемой, если тройка $ (f_2, \mathcal{L}(X_1), \mathcal{L}(N_2)) $ не является решением дифференциального уравнения
$$ \xi''' = \xi'' \left( -\frac{\nu''' f'}{\nu''} + \frac{f''}{f'} \right) - 2 \nu'' f'' f' + \nu' f''' + \frac{\nu' \nu''' f'' f'}{nu''} - \frac{nu' (f'')^2}{f'}, $$
где $ f = f_2, \xi = \log{p_{X_1}, \nu = \log{p_{N_2}} $.

Идентифируемость графа, лежащего в основе исследуемой модели, гарантируется следующей теоремой.

Теорема. Пусть $ \mathcal{L}(X) = \mathcal{L}(X_1, X_2) $ порождена идентифицируемой моделью двух переменных с аддитивным шумом с графом $ \mathcal{G}_0 $ и выполняется условие причинной минимальности. Тогда $ \mathcal{G}_0 $ идентифицируем по совместному распределению.

Результаты, полученные для модели двух переменных, могут быть обобщены для случая нескольких переменных.

#### Модели с аддитивным шумом нескольких переменных

Пусть теперь заданы $ p $ уравнений вида $ X_j = f_j(PA_j) + N_j $.

Определение. Будем называеть SEM ограниченной моделью с аддитивным шумом, если для всех $ j \in V, i \in PA_j $ и множеств $ S \subset V $, для которых $ PA_j \ { i } \subset S \subset ND_j \ {i, j} $, существует $ x_S: p_s(x_S) > 0 $ такой, что $$ \left( f_j(x_{PA_{j} \ {i}}, \cdot), \mathcal{L}(X_i | X_S = x_S), \mathcal{L}(N_j) \right) $$ выполняет условие 1.

Теорема. Пусть $ \mathcal{L}(X) = \mathcal{L}(X_1, \dots, X_p) $ порождено ограниченой моделью с аддитивным шумом и графом $ \mathcal{G}_0 $ и выполняет условие причинной минимальности относительно $ \mathcal{G}_0 $ (т.\,е. функции $ f_j $ не постоянны. Тогда $ \mathcal{G}_0 $ идентифицируем по совместному распределению.

### Алгоритм RESIT (Regression with Subsequent Independence Test)

На практике имеется конечный набор i.i.d. данных, полученный из совместного распределения, по которому требуется оценить граф, порождающий это распределение.

Алгоритм опирается не следующий факт: для каждой вершины $ X_i $ соответствующая шумовая переменная $ N_i $ не зависит от всех не-потомков $ X_i $. Предлагается следующая итерационная процедура: на каждом шаге обнаруживается и удаляется из рассмотрения вершина-сток (вершина, из которой нет исходящих рёбер). Это осуществляется путём регрессии каждой из оставшихся вершин (переменных) на все другие переменных и оценки меры зависимости между остатками и переменными. Переменная, для которой получена минимальная оценка зависимости остатков, считается стоком.

Первый этап алгоритма определяет причинное упорядочивание вершин полного графа, на втором этапе у каждой вершины удаляются лишние входящие рёбра до тех пор, пока остатки регрессии не станут независимыми.

Алгоритм предоставляет выбор способов построения модели регрессии и тестирования независимости отстатков. Авторы рекомендуют использовать модели линейной и нелинейной регрессии (GAM), а также Gaussian
process regression, а для меры зависимости — p-значение теста независимости [Hilbert-Schmidt independence criterion](https://papers.nips.cc/paper/3201-a-kernel-statistical-test-of-independence.pdf).

![Algo](http://i.imgur.com/UV55OuQ.png)

Теорема. Пусть $ \mathcal{L}(X) = \mathcal{L}(X_1, \dots, X_p) $ порождено ограниченной моделью с аддитивным шумом и графом $ \mathcal{G}_0 $ и выполняется условие причинной минимальности относительно графа $ \mathcal{G}_0 $. Тогда алгоритм RESIT, использующий соответствующую модель регрессии и «оракул» независимости (метод, с абсолютной точностью определяющий независимость входных аргументов) гарантированно находит правильный граф $ \mathcal{G}_0 $ по совместному распределению $ \mathcal{L}(X) $.

Алгоритм выполняет порядка $ O(p^2) $ тестов независимости (полиномиальная сложность в зависимости от количества вершин). Это становится вычислительно сложным для графов с большим количеством переменных. Помимо прочего, на практике в нашем распоряжении нет оракула, определяющего независимость, поэтому её придётся определять из конечных наборов данных. Для приемлемой точности определение независимости в задачах большой размерности потребует быстро растущих объёмов выборки.

### Существующие алгоритмы

PC-алгоритм (Peter-Clark) находит частично ориентированное остовное дерево причинного графа, соответствующего классу марковской эквивалентности истинного графа. Алгоритм основан на тестировании условной независимости (CIB, conditional independence-based), его описание можно найти в книге "Causation, Prediction, and Search" Peter Spirtes, Clark Glymour, and Richard Scheines, Chapter 25. Имеется реализация алгоритма на языке R в пакете `pcalg` (функция `pc`). Такие методы различают причинные графы до класса марковской эквивалентности, поэтому некоторые рёбра могут остаться ненаправленными.

Score-Based методы вводят специальную оценку соответствия графа распределению $ S(\mathcal{D}, \mathcal{G}) $, а затем некоторым образом максимизируют её:  $$ \underset{\mathcal{G} \text{ DAG на } X }{\arg\!\max} S(\mathcal{D}, \mathcal{G}) $$ .

Метод LiNGAM использует ограничения на распределение шумовых переменных.

Теорема (Peters and Bühlmann, 2014). Пусть SEM с графом $ \mathcal{G}_0 $ состоит из
$$ X_j = \sum_{k \in PA_{j}^{\mathcal{G}_0}} \beta_{jk} X_k} + N_j, \quad j = 1, \dots, p, $$
где все $ N_j $ независимы в совокупности и распределены не нормально, и для $ j \in \{ 1, \dots, p \} \quad \beta_{jk} \ne 0 $ для всех $ k \in PA_{j}^{\mathcal{G}_0} $. Тогда граф $ \mathcal{G}_0 $ идентифицируем по совместному распределению.

### Тестирование на модельных данных

Авторы проверяют точность работы алгоритма на тестовых наборах данных, сгенерированных для линейных и нелинейных $ f_i $ в SEM. Эксперименты показывают, что RESIT оказывается лучше метода PC, а в случае $ p = 15 $ переменных справляется с определением структуры графа для нелинейных SEM.

Упрощённая версию алгоритма была протестирована на [коллекции пар «причина—следствие»](webdav.tuebingen.mpg.de/cause-effect/), полученных из различных предметных областей. Для каждой пары требовалось указать, какая переменная является непосредственной причиной другой. (Похожим образом была поставлена задача в одном из соревнований Kaggle: https://www.kaggle.com/c/cause-effect-pairs/.) Предложенный алгоритм показал точность около 80% на наборе из 86 пар.