[Исходная статья: Identifiability of Causal Graphs using Functional Models](http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/2011/UAI-2011-Peters.pdf) - *Отличается от первоначально выбранной!*

Популярным средством измерения силы причинно-следственной связи процессов, протекающих в природе или обществе, является корреляция. Однако существует принцип, который формулируется довольно просто: "correlation does not mean causation". Пренебрежение им может порождать так называемые «ложные корреляции»: например, количество гнёзд аистов влияет на рождаемость в европейских странах или количество фильмов, в которых снялся Николас Кейдж, может быть каким-то образом связано с гибелью людей в бассейнах.
![Storks](http://i.imgur.com/sKwYC0Y.png)
![Nicolas Cage](http://i.imgur.com/DvqDAbM.png)

Возникает естественный вопрос: в каких случаях "correlation DOES mean causation"? На этот вопрос пытается ответить [группа исследователей из Института интеллектуальных система Макса Планка.](https://ei.is.tuebingen.mpg.de/research_groups/causal-inference-group) Оказывается, при выполнении некоторых условий, можно восстановить причинно-следственную связь из результатов наблюдений.

Для исследования причинно-следственных связей между процессами [Джуда Пёрл](https://ru.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D0%BB,_%D0%94%D0%B6%D1%83%D0%B4%D0%B0) предложил фунцкиональные причинные модели (functional causal models). По уравнениям, описывающим причинную модель, можно построить граф, который отображает зависимости между случайными величинами, но не описывает характера таких зависимостей. Таким образом, граф содержит меньше информации о модели, однако позволяет ввести в терминах теории графов некоторые критерии, упрощающие определение условной независимости между переменными. Эти понятия неразрывно связаны с понятием байесовских сетей. В 2011 году Перл стал лауреатом Премии Тьюринга за «фундаментальный вклад в искусственный интеллект посредством разработки исчисления для проведения вероятностных и причинно-следственных рассуждений».

PC-алгоритм (Peter-Clark) находит частично ориентированное остовное дерево причинного графа, соответствующего классу марковской эквивалентности истинного графа. Алгоритм основан на тестировании условной независимости (CIB, conditional independence-based), его описание можно найти в книге "Causation, Prediction, and Search" Peter Spirtes, Clark Glymour, and Richard Scheines (2001). Имеется реализация алгоритма на языке R в пакете `pcalg` (функция `pc`).

### Определения
Ориентированный ациклический граф (DAG, directed acyclic graph) — это ориентированный граф, в котором отсутствуют направленные циклы, то есть пути, начинающиеся и кончающиеся в одной и той же вершине.

Пусть $ (X_i)_{i \in V} $ — семейство случайных величин, $ G = (V, E) $ — DAG с вершинами $ E \subseteq V^2 $. Как правило, в литературе используют обозначения $ X_i $ и $ i $ для соответствующих вершин графа.

$ X_i $ называется вершиной-родителем вершины $ X_j $ если $ (i, j) \in E $ и вершиной-ребёнком, если $ (j, i) \in E $. Множество вершин-родителей вершины $ X_j $ обозначатся $ pa_j $, множество вершин-детей — $ ch_j $.

Если $ (i, j) \in E $ и $ (k, j) \in E $, $ j $ называется *коллайдером*.

Направленным путём в графе $ G $ называется последовательность различных вершин $ X_{i_1}, ..., X_{i_n} $, для которых $ (i_k, i_{k+1}) \in E $ или $ (i_{k+1}, i_k) \in E $ для $ k = 1, ..., n - 1 $. Если между вершинами $ X_{i_1} $ и $ X_{i_n} $ есть направленный путь, вершина $ X_{i_n} $ называется потомком вершины $ X_{i_1} $. Множество всех потомков вершины $ X_i $ обозначается $ de_i $, множество вершин-не-потомков — $ nd_i $.

Путь между $ X_{i_1} $ и $ X_{i_n} $ называется заблокированным множеством $ S $ (не включая ни $ X_{i_1} $, ни $ X_{i_n} $), если существует такая вершина $ X_{i_k} $, что выполняется одно из условий:
1. $ X_{i_k} \in S $ и
  - $ X_{i_{k-1}} \rightarrow X_{i_k} \rightarrow X_{i_{k+1}} $ или
  - $ X_{i_{k-1}} \leftarrow X_{i_k} \leftarrow X_{i_{k+1}} $ или
  - $ X_{i_{k-1}} \leftarrow X_{i_k} \rightarrow X_{i_{k+1}} $
2. $ X_{i_{k-1}} \rightarrow X_{i_k} \leftarrow X_{i_{k+1}} $ и ни $ X_{i_k} $, ни её потомки не принадлежат $ S $.

Говорят, что два непересекающихся множества вершин $ A $ и $ B $ d-разделены (непересекающимся с ними множеством $ S $), если каждый путь между вершинами $ A $ и $ B $ заблокирован множеством $ S $.

Совместное распределение $ \mathbb{P}^{(X_i)_{i \in V}} $ называется марковым относительно DAG $ G $, если $ A $ и $ B $ d-разделены $ C $ $ \Rightarrow A ⫫ B \mid C $ для всех непересекающихся множеств $ A, B, C $.

$ \mathbb{P}^{(X_i)_{i \in V}} $ называется достоверным (faihful) относительно DAG G, если $ A, B $ d-разделены $ C $ $ \Leftarrow A ⫫ B \mid C $ для всех непересекающихся множеств $ A, B, C $.

Два графа называются марковски эквивалентными, если в них выполняются одинаковые наборы d-разделимости, т. е. имеют одинаковые наборы условных независимостей.

_TODO: примеры эквивалентности, faihfulness_

### $ \mathscr{F}-FMOC $
Функциональной моделью (functional model, structural equation model) называются $ V $ уравнений вида $$ X_i = f_i(pa_i, N_i), \quad i \in V, $$
где $ pa_i \subset V \setminus { X_i } $ и $ (N_i)_{i \in V} $ взаимно независимо распределены по $ \mathbb{P}^{N_i} $, и граф, полученный построением направленных рёбер от элементов $ pa_i $ к $ X_i $, $ i \in V $, ацикличен.

$ \mathscr{F}-FMOC $ (functional model class with function class $ \mathscr{F} $) называется классом функциональных моделей с классом функций $ \mathscr{F} $, если для заданного класса функций $ \mathscr{F} \subset {f \mid f : \mathbb{R}^m \rightarrow \mathbb{R}, 2 \leqslant m \leqslant \left|V\right| } $ $ f_i \in \mathscr{F}, i \in V $ каждая модель порождает абсолютно непрерывную относительно меры Лебега $ \mathbb{P}^{(X_i), i \in V} $.

Каждая функциональная модель порождает уникальное совместное распределение $ \mathbb{P}^{(X_i), i \in V} $.

### Факторизация
Граф, полученный из функциональной модели, позволяет факторизовать совместную плотность распределения по следующей формуле:

$$ \mathbb{P}^{(X_i), i \in V} = p(X_1, ..., X_n) = \prod_j p(X_j \mid pa_j) $$

### Математическая постановка задачи.

Пусть $ (X_i)_{i \in V} $ — конечный набор случайных величин.

**Задача**: по i.i.d. выборкам из совместного распределения $ \mathbb{P}^{(X_i)_{i \in V}} $ восстановить DAG $ G_c $ процесса, породившего эти данные.

Похожим образом была поставлена задача в одном из соревнований Kaggle: https://www.kaggle.com/c/cause-effect-pairs/. В нём участникам предлагалось по заданным парам определеить, что из них является причиной, а что — следствием. Эти наборы данных были получены из различных предметных областей из процессов, в которых причинно-следственные связи уже известны. Чтобы усложнить задачу, к данным были добавлены наборы зависимых, но не являющихся непосредственными причинами друг друга процессов, а также искуственно сгенерированные пары.

Согласно теореме, приведённой в статье, структура причинного графа может быть определена из совместного распределения с точностью до класса (марковской) эквивалентности, если выполняются условия марковости и достоверности. В этих условиях работает, например, алгоритм PC.

**Лемма** Можно показать, что следующие пары могут быть различены ($ \tilde{m} \in \mathbb{N} $):
1. (линейные модели с аддитивным шумом) $ \mathscr{F}_1 = { f(x, n) = ax + n} $;
2. (дискретные модели с аддитивным шумом) $ \mathscr{F}_1 = { f(x, n) = \varphi(x) + n \mod \tilde{m}} $;
3. (нелинейные модели с аддитивным шумом) $ \mathscr{F}_1 = { f(x, n) = \varphi(x) + n} $.

**Теорема** Пусть $ \mathbb{P}^{(X_i), i \in V} $ порождено функциональной моделью из $ \mathscr{F}-IFMOC $ с графом $ G $. Тогда оно не может быть порождено функциональной моделью из $ \mathscr{F}-IFMOC $, которому соответствует другой граф $ G' \ne G $.

**Следствие:** Если процесс, порождающий выборку, принадлежит $ \mathscr{F}-IFMOC $ с графом $ G = G_c $ (т. е. $ pa_i $ являются непосредственными причинами $ X_i $), то истинный причинный DAG $ G_c $ может быть восстановлен по совместному распределению $ \mathbb{P}^{(X_i), i \in V} $.

### Алгоритм

Алгоритм обучает функциональную модель для всех возможных конфигураций графа и выводит те из них, для которых остатки оказываются независимыми от регрессоров.

_$ \sigma_1, ..., \sigma_d $ задают порядок._

_Что такое $ \sigma_i $?_

Алгоритм не указывает, какую модель регрессии следует обучать в процессе работы. Конкретный вид модели следует подбирать из дополнительных знаний о природе наблюдаемого процесса. Кроме того, следует выбрать критерий определения независимости двух выборок.

![Algo 1](http://i.imgur.com/ZOYW8bi.png)
![Algo 2](http://i.imgur.com/pPYipwA.png)

В случаях, когда нет подходящих графов, соответствующих причинной структуре данных, или их больше одного, алгоритм полагает, что нарушены условия теоремы 2, и сообщает, что он не способен выбрать определённую модель для вывода в качестве результата.

Разумеется, конечная выборка не является точным описанием $ \mathbb{P}^{(X_i)_{i \in V}} $. Авторы статьи не приводят оценок качества работы алгоритма в зависимости от размера входной выборки.

### Сравнение с модельными данными
Авторы проверяют точность работы алгоритма на тестовых наборах данных, сгенерированных i.i.d. из моделей, состоящих из четырёх уравнений, и сравнивают результаты работы с алгоритмом PC.

Авторы использовали модели линейной регрессии и [Gaussian Process](http://gaussianprocess.org/gpml/), а для тестирования независимости остатков использовали Hilbert-Schmidt independence criterion. 

В случаях, когда выполняются требования марковости и достоверности выполняются, предлагаемый авторами алгоритм совершает меньше ошибок. Если требование достоверности не выполняется, алгоритм PC всегда ошибается, в то время как предложенный алгоритм продолжает выдавать корректные результаты. Если оба требования нарушены, алгоритм PC выдаёт неправильные результаты; предложенный алгоритм сообщает, что не способен определить структуру.

