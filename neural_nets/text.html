<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="russian" xml:lang="russian">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Анастасия Миллер" />
  <title>Нейронные сети: принципы, проблемы, что работает, что нет и почему.</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title"><p>Нейронные сети: принципы, проблемы, что работает, что нет и почему.</p></h1>
<h2 class="author">Анастасия Миллер</h2>
</div>
<div id="TOC">
<ul>
<li><a href="#про-мозг-передача-информации-путём-активации-нейронов.">Про мозг: передача информации путём активации нейронов.</a></li>
<li><a href="#перцептрон">Перцептрон</a></li>
<li><a href="#архитектура-полносвязной-сети-и-передача-информации-в-ней.">Архитектура полносвязной сети и передача информации в ней.</a><ul>
<li><a href="#нейронная-сеть-универсальный-аппроксиматор">Нейронная сеть – универсальный аппроксиматор</a></li>
</ul></li>
<li><a href="#задача-обучения-сети">Задача обучения сети</a></li>
<li><a href="#решение-задачи-обучения-сети">Решение задачи обучения сети</a><ul>
<li><a href="#метод-градиентного-спуска">Метод градиентного спуска</a></li>
<li><a href="#алгоритм-обратного-распространения-ошибки">Алгоритм обратного распространения ошибки</a></li>
</ul></li>
<li><a href="#примеры-неглубоких-сетей">Примеры неглубоких сетей</a><ul>
<li><a href="#autoencoder">Autoencoder</a></li>
<li><a href="#word2vec">Word2Vec</a></li>
</ul></li>
<li><a href="#проблемы">Проблемы</a></li>
<li><a href="#решения">Решения</a><ul>
<li><a href="#convolutional-neural-networks">Convolutional neural networks</a></li>
<li><a href="#local-vs.global-optimization">Local vs. global optimization</a></li>
<li><a href="#optional-recurrent-neural-networks">[optional] Recurrent neural networks</a></li>
</ul></li>
</ul>
</div>
<h2 id="про-мозг-передача-информации-путём-активации-нейронов.">Про мозг: передача информации путём активации нейронов.</h2>
<p>Идея, стоящая за созданием нейронных сетей, заключается в том, чтобы промоделировать работу человеческого мозга. Очень общее и условное представление об устройстве и работе клетки человеческого мозга и легло в основу математической модели “нейронная сеть”. Естественный нейрон в коре головного мозга, согласно этому представлению, выглядит следующим образом:</p>
<p>Клетка имеет множество разветвлённых отростков — дендритов, и одно длинное тонкое волокно — аксон, на конце которого находятся синапсы, примыкающие к дендритам других нервных клеток. Каждая нервная клетка может находиться в двух состояниях: обычном и возбуждённном. В возбуждённом состоянии клетка генерирует электрический импульс, который проходит по аксону до синапсов. Синапс при приходе импульса выделяет вещество, способствующее проникновению положительных зарядов внутрь соседней клетки. Синапсы имеют разную способность концентрировать это вещество, некоторые даже препятствуют его выделению — они называются тормозящими. Если суммарный заряд, попавший в клетку, превосходит некоторый порог, клетка возбуждается и генерирует импульс, который распространяется по аксону и доходит до синапсов, что способствует возбуждению следующих клеток. После возбуждения клетки наступает период релаксации — некоторое время она не способна генерировать новые импульсы. Благодаря этому клетки работают по тактам, наподобие дискретных автоматов, а сеть в целом передаёт направленную волну импульсов. <span class="citation">(Воронцов <a href="#ref-Vorontsov2007">2007</a>)</span></p>
<h2 id="перцептрон">Перцептрон</h2>
<p>Естественной формализацией этого описания является следующая модель:</p>
<p>Пусть <span class="math inline">\(\mathcal X\)</span> — пространство объектов; <span class="math inline">\(\mathcal Y\)</span> — множество допустимых ответов; <span class="math inline">\(y^*: \mathcal X \to \mathcal Y\)</span> — целевая зависимость, известная только на объектах обучающей выборки <span class="math inline">\(X = \left\{ \left(x^{(i)}, y_i\right) \right\}_{i = 1}^\ell, \forall i \in 1\mathbin{:}\ell \:y_i = y^*(x^{(i)})\)</span>. Требуется построить алгоритм <span class="math inline">\(a: X \to Y\)</span>, аппроксимирующий целевую зависимость <span class="math inline">\(y^*\)</span> на всём множестве <span class="math inline">\(\mathcal X\)</span>.</p>
<p>Будем предполагать, что объекты описываются <span class="math inline">\(n\)</span> числовыми признаками <span class="math inline">\(\left(x^{(i)}_1, \ldots, x^{(i)}_n\right)\)</span> (такой вектор называется <em>признаковым описанием</em> объекта <span class="math inline">\(x^{(i)}\)</span>).</p>
<p>Значения этих признаков будем трактовать как величины импульсов, поступающих на вход нейрона через <span class="math inline">\(n\)</span> входных синапсов. Поступающие в нейрон импульсы складываются с весами <span class="math inline">\(w_1, \ldots, w_n\)</span>. Если вес положительный, то соответствующий синапс возбуждающий, если отрицательный, то тормозящий. Если суммарный импульс превышает заданный порог активации <span class="math inline">\(w_0\)</span>, то нейрон возбуждается и выдаёт на выходе 1, иначе выдаётся 0. То есть нейрон вычисляет следующую функцию:</p>
<p><span class="math display">\[a(x) = \varphi\left(\sum_{j=1}^n w_j x_j - w_0\right), \text{ где } \varphi(z) = \begin{cases}1, z &gt; 0, \\ 0, z \leq 0\end{cases}.\]</span></p>
<p>В теории нейронных сетей функцию <span class="math inline">\(\varphi\)</span> принято называть <em>активационной функцией</em> нейрона.</p>
<p>Эта модель естественным образом расширяется: <span class="math inline">\(\varphi\)</span> может быть не ступенчатой функцией, а, например, сигмоидальной (<span class="math inline">\(\sigma(z) = 1 / \left(1 + e^{-z}\right)\)</span>), просто линейной (<span class="math inline">\(\mathrm{id}(z) = z\)</span>) или обрезанной линейной (<span class="math inline">\(\mathrm{ReLU}(z) = \max\left\{0, z\right\}\)</span>). В общем случае эта модель называется (однослойным) перцептроном.</p>
<p>Согласно той же биологический мотивации способность синапсов пропускать заряд (обозначенная в модели весами) изменяется в зависимости от того, какую задачу решает данный нейрон. Например, нейрон, сравнивающий два своих входа <span class="math inline">\(x_1\)</span> и <span class="math inline">\(x_2\)</span> и выдающий 1, если <span class="math inline">\(x_1 &gt; x_2\)</span>, 0 в обратном случае, будет иметь вектор весов <span class="math inline">\(\left(0, 1, -1\right)\)</span> (или любой другой пропорциональный): см. рис . А линейный классификатор вида <span class="math inline">\(a(x_1, x_2) = \begin{cases} 1, \text{если} 2 x_1 + 3 x_2 &gt; 5, \\ 0\:\text{иначе}\end{cases}\)</span> представим в виде нейрона с весами (-5, 2, 3): см. рис. .</p>
<p><img src="perceptron_scheme_1.png" alt="Перцептрон, реализующий функцию \max" style="width:70.0%" /> <img src="perceptron_sample_1.png" alt="Перцептрон, реализующий функцию \max" style="width:70.0%" /></p>
<p><img src="perceptron_scheme_2.png" alt="Перцептрон, реализующий a(x_1, x_2) = 1, \text{ если } 2 x_1 + 3 x_2 &gt; 5, 0 \text{ иначе }" style="width:70.0%" /> <img src="perceptron_sample_2.png" alt="Перцептрон, реализующий a(x_1, x_2) = 1, \text{ если } 2 x_1 + 3 x_2 &gt; 5, 0 \text{ иначе }" style="width:70.0%" /></p>
<h2 id="архитектура-полносвязной-сети-и-передача-информации-в-ней.">Архитектура полносвязной сети и передача информации в ней.</h2>
<p>Следующая идея состоит в том, что нейроны можно комбинировать. Ведь правда же, ничто не мешает нам результат вычисления одного нейрона рассматривать как входное значение для другого нейрона (как оно и происходит в мозгу)<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<p>На рис.  показана вычислительная схема двуслойной полносвязной нейронной сети. В ней можно выделить несколько похожих конструкций:</p>
<ul>
<li>узлы <span class="math inline">\(a^1_1\)</span>, <span class="math inline">\(a^1_2\)</span> и <span class="math inline">\(a^1_3\)</span> являются входами, а узел <span class="math inline">\(a^2_1\)</span> — выходом перцептрона с весами <span class="math inline">\(w^2_{11}, w^2_{12}, w^2_{13}\)</span> и суммирующей функцией <span class="math inline">\(z^2_1\)</span>;</li>
<li>узлы <span class="math inline">\(a^1_1\)</span>, <span class="math inline">\(a^1_2\)</span> и <span class="math inline">\(a^1_3\)</span> являются входами, а узел <span class="math inline">\(a^2_2\)</span> — выходом перцептрона с весами <span class="math inline">\(w^2_{21}, w^2_{22}, w^2_{23}\)</span> и суммирующей функцией <span class="math inline">\(z^2_2\)</span>;</li>
<li>узлы <span class="math inline">\(a^2_1\)</span> и <span class="math inline">\(a^2_1\)</span> являются входами, а узел <span class="math inline">\(a^3_1\)</span> — выходом перцептрона с весами <span class="math inline">\(w^3_{11}, w^3_{12}\)</span> и суммирующей функцией <span class="math inline">\(z^3_1\)</span>.</li>
</ul>
<div class="figure">
<img src="abstract_net_example.png" alt="изображение абстрактной нейронной сети" style="width:50.0%" />
<p class="caption">изображение абстрактной нейронной сети</p>
</div>
<p>Здесь уже куда-то “потерялись” смещения для перцептронов. Обычно их не изображают на схемах, так как это занимает лишнее место, но в вычислениях они, конечно, всё равно присутствуют. Если смещения не “схлопывают” в дополнительный вход, который всегда равен единице (посмотрите на картинку  и заметьте, что вход, обозначенный как “Смещение”, отличается от “Входа #1” и “Входа #2” только тем, что мы знаем его значение заранее и это значение постоянно), то обозначают смещение <span class="math inline">\(j\)</span>-го нейрона на <span class="math inline">\(l\)</span>-ом уровне как <span class="math inline">\(b^l_j\)</span>. Смещения меняются так же, как и веса <span class="math inline">\(w^l_{jk}\)</span>, и если мы пишем “минимизация по множеству весов”, то смещения в это множество входят.</p>
<p>В этих обозначениях есть некоторая двойственность. В зависимости от контекста <span class="math inline">\(z^i_j\)</span> и <span class="math inline">\(a^i_j\)</span> могут означать как сумматорную или активационную функцию, так и её значение на конкретных входных данных.</p>
<p>Таким образом, полносвязная нейронная сеть вычисляет следующую рекуррентную функцию:</p>
<p><span class="math display">\[a^1_ i = \mathtt{input[}i\mathtt{]}\]</span> <span class="math display">\[a^l_ i = a^l_ i(z^l_ i) = a^l_ i(\sum_ j w^l_ {ij} a^{l-1}_ j + b^l_ j)\]</span></p>
<p>(здесь я уже подставила в качестве <span class="math inline">\(z^l_i\)</span> обычную суммирующую функцию). Активационная функция может быть какой угодно, обычно выбирается в зависимости от требований к области значений ответов: на внутренних слоях сигмоида, на наружных — сигмоида, арктангенс или ReLU. Почему такие — сейчас скажем.</p>
<p>Обобщая эту конструкцию, получаем систему, которую мы видели на первой картинке: <span class="math display">\[a^l_ i = a^l_ i \left(\sum_j w^l_ {ij} a^{l-1}_ j + b^l_ j\right)\]</span></p>
<h3 id="нейронная-сеть-универсальный-аппроксиматор">Нейронная сеть – универсальный аппроксиматор</h3>
<p>Давайте теперь детально поговорим о том, какие именно функции нейросеть теоретически может вычислить. <span class="citation">(Воронцов <a href="#ref-Vorontsov2011">2011</a>)</span></p>
<p>Для однослойной сети у нас есть <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">универсальная аппроксимационная теорема</a>:</p>
<p>T.<span class="citation">(Cybenko <a href="#ref-cybenko1989">1989</a>)</span>: Пусть <span class="math inline">\(\varphi\)</span> – непрерывная ограниченная монотонно возрастающая функция, не являющаяся константой. Тогда <span class="math inline">\(\forall f \in C\left(\left[0;1\right]^m\right)\)</span> (множества непрерывных в <span class="math inline">\(\left[0;1\right]^m\)</span> функций) и <span class="math inline">\(\varepsilon &gt; 0\)</span> <span class="math inline">\(\exists\: N\in\mathbb{N}, \left\lbrace v_i, b_i\in\mathbb{R}, w_i\in\mathbb{R}^m\right\rbrace_{i=1}^N\)</span> такие, что функция <span class="math display">\[F(x) = \sum_{i=1}^N v_i \varphi(w_i^\mathrm{T} x + b_i)\]</span> близка к <span class="math inline">\(f\)</span> в том смысле, что <span class="math inline">\(\forall x \in \left[0;1\right]^m \: \left\vert F(x) - f(x) \right\vert &lt; \varepsilon\)</span>. Иными словами, множество функций вида <span class="math inline">\(F(x)\)</span> плотно в единичном <span class="math inline">\(m\)</span>-мерном кубе.</p>
<p>Функции вида <span class="math inline">\(F(x)\)</span> – это именно то, что вычисляется перцептроном. То есть для любой непрерывной функции существует сколь угодно точное представление в виде перцептрона. Впрочем, доказательство теоремы не разрешает проблему нахождения этого перцептрона и даже не показывает, насколько широким он должен быть (какое должно быть <span class="math inline">\(N\)</span>).</p>
<p>Поэтому достаточно естественным является следующий вопрос: а не даст ли нам многослойная нейронная сеть болеее быстрого результата? Обычно здесь даётся мотивационный пример: попробуйте сконцентрировать перцептроны с двумя входами, реализующие функции “и”, “или” и “исключающее или”. <sub><sup>Спойлер: “исключающее или” представить в виде однослойного перцептрона не получится.</sup></sub></p>
<p>Для многослойной сети тоже было доказано подобие универсальной аппроксимационной теоремы:</p>
<p>Опр.: Набор функций <span class="math inline">\(F \subset C(X)\)</span> называется замкнутым относительно функции <span class="math inline">\(\phi : \mathbb{R} \to \mathbb{R}\)</span>, если для любого <span class="math inline">\(f \in F\)</span> выполнено <span class="math inline">\(\phi(f) \in F\)</span>.</p>
<p>Т. <span class="citation">(Горбань и др. <a href="#ref-Gorban1998">1998</a>)</span>: Пусть <span class="math inline">\(X\)</span> — компактное пространство, <span class="math inline">\(C(X)\)</span> — алгебра непрерывных на <span class="math inline">\(X\)</span> вещественных функций, <span class="math inline">\(F\)</span> — линейное подпространство в <span class="math inline">\(C(X)\)</span>, замкнутое относительно нелинейной непрерывной функции <span class="math inline">\(\phi\)</span>, содержащее константу (<span class="math inline">\(1 \in F\)</span>) и разделяющее точки множества <span class="math inline">\(X\)</span>. Тогда <span class="math inline">\(F\)</span> плотно в <span class="math inline">\(C(X)\)</span>.</p>
<p>То есть для любой функции <span class="math inline">\(g \in C(X)\)</span> можно взять одну нелинейную <span class="math inline">\(\phi\)</span> и суперпозицией её линейных комбинаций (которой и является нейросеть, если у всех нейронов активационные функции одинаковы) приблизить <span class="math inline">\(g\)</span> с любой заданной точностью. Конструктивного способа выбрать необходимую суперпозицию линейных комбинаций, впрочем, доказательство теоремы всё равно не представляет.</p>
<h2 id="задача-обучения-сети">Задача обучения сети</h2>
<p>У нас есть некоторая непрерывная функция <span class="math inline">\(f: \mathbb{R}^N \to \mathbb{R}^k\)</span>, вида которой мы не знаем, а знаем только её значение на конечном наборе точек. Мы хотим построить её равномерное приближение.</p>
<p>NB: вообще-то задача классификации (когда у <span class="math inline">\(f\)</span> конечное множество значений) решается сетями едва ли не чаще, чем задача регрессии, но с теоретическими обоснованиями здесь хуже.</p>
<p>Вообще мы умеем решать <a href="#задача-обучения-сети">эту задачу</a> с помощью многочленов: любой конечный набор точек можно приблизить многочленом соответствующей степени, говорит нам <a href="https://ru.wikipedia.org/wiki/Теорема_Вейерштрасса_—_Стоуна#.D0.A2.D0.B5.D0.BE.D1.80.D0.B5.D0.BC.D0.B0_.D0.92.D0.B5.D0.B9.D0.B5.D1.80.D1.88.D1.82.D1.80.D0.B0.D1.81.D1.81.D0.B0">теорема Вейерштрасса-Стоуна</a>. Но приближение многочленами плохо обобщает зависимость (тут вспоминаем “колбасящиеся” между точками многочлены, картинку сами рисуйте).</p>
<p>Будем строить приближение нейронной сетью с фиксированной архитектурой<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>, то есть зафиксируем количество слоёв <span class="math inline">\(L\)</span>, количество нейронов на каждом слое <span class="math inline">\(n_l, l\in 1\ldots L\)</span><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> и все сумматорные и активационные функции <span class="math inline">\(z^l_j, a^l_j\)</span> для <span class="math inline">\(j \in 1\ldots n_l, l\in 1\ldots L\)</span> Функция <span class="math inline">\(a\)</span>, вычисляемая сетью при зафиксированной архитектуре сети, зависит только от множества её весов: <span class="math inline">\(a = a\left(\left\{w_{ij}^l\right\}\right)\)</span> в том смысле, что в зависимости от набора весов <span class="math inline">\(\left\{w_{ij}^l\right\}\)</span> мы будем вычислять разные функции. Тогда наша задача задаче настройки весов сети, которая заключается в том, чтобы найти набор весов</p>
<p><span class="math display">\[\{w_{ij}^l\} = \mathrm{arg\,min}_{\{w_{ij}^l\}} J\left(f, a\left(\left\{w_{ij}^l\right\}\right)\right),\]</span></p>
<p>где <span class="math inline">\(J\)</span> – функция, измеряющая расстояние между <span class="math inline">\(f\)</span> и <span class="math inline">\(a\left(\left\{w_{ij}^l\right\}\right)\)</span>.</p>
<p>Функция <span class="math inline">\(f\)</span> нам неизвестна, известен лишь набор её значений в некоторых точках (то бишь примеры входных данных и правильных ответов для них). Поэтому подгоняться веса сети будут так, чтобы приблизить значения функции в известных нам точках.</p>
<p>Возможно, будет проще осознать предыдущие параграфы, если представить в них <span class="math inline">\(J(f, a) = \frac{1}{N}\sum_{i=1}^N \left\Vert f(x_i) - a\left(\left\{w_{ij}^l\right\}\right)(x_i)\right\Vert^2\)</span>, то есть стандартную mean square error. Здесь можно заметить, что задача логистической регрессии на <span class="math inline">\(p\)</span> предикторов эквивалентна задаче, решаемой перцептроном с сигмоидальной активационной функцией на <span class="math inline">\(p\)</span> входах.</p>
<h2 id="решение-задачи-обучения-сети">Решение задачи обучения сети</h2>
<p>Ещё раз посмотрим на задачу. Для того, чтобы уменьшить количество обозначений, сумматорную функцию будем считать обычной суммой и подставим сразу MSE в качестве целевой функции. Понятно, что целевая функция может быть любой и может (и должна) меняться от задачи к задаче.</p>
<p><span class="math display">\[\{w_{ij}^l\} = \mathrm{arg\,min}_{\{w_{ij}^l\}} \frac{1}{N}\sum_{i=1}^N \left\Vert f(x_i) - a\left(\left\{w_{ij}^l\right\}\right)(x_i)\right\Vert^2,\]</span></p>
<p>где <span class="math inline">\(x_i = \left(x_i^{(1)}, \ldots, x_i^{(m)}\right) \in \mathbb{R}^m\)</span> – <span class="math inline">\(i\)</span>-й элемент обучающего набора данных, <span class="math inline">\(f(x_i) = y_i\)</span> – “правильный ответ”, значение искомой функции на <span class="math inline">\(x_i\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
a_j^1\left(x_i\right) &amp;= x_i^{(j)}, j\in 1\ldots m, \\
a_j^l\left(x_i\right) &amp;= a_j^l\left(\sum_{k = 1}^{n_{l-1}} w^l_{jk} a^{l-1}_k \left(x_i\right) + b^l_j\right), j\in 1\ldots n_l, l\in 2\ldots L, \\
a\left(\left\{w_{ij}^l\right\}\right)(x_i) &amp;= \left(a_1^L\left(x_i\right), \ldots, a_{n_L}^L\left(x_i\right)\right).
\end{aligned}\label{eq:chain_def}\]</span><a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<p>Пример. В случае, когда <span class="math inline">\(L = 2, n_L = 1\)</span> и <span class="math inline">\(a_1^2 = \mathrm{id}\)</span>, мы получаем задачу линейной регрессии: <span class="math display">\[\{w_i, b\} = \mathrm{arg\,min}_ {\{w_ i, b\}} \frac{1}{N}\sum_{i=1}^N \left( \sum_{j=1}^m w_j x_i^{(j)} + b - f(x_i)\right)^2,\]</span> для которой точка минимума находится аналитически. А если при тех же условиях <span class="math inline">\(a_1^2 = \sigma\)</span>, то мы получаем задачу логистической регрессии: <span class="math display">\[\{w_i, b\} = \mathrm{arg\,min}_ {\{w_ i, b\}} \frac{1}{N}\sum_{i=1}^N \left(\sigma\left( \sum_{j=1}^m w_j x_i^{(j)} + b \right) - f(x_i)\right)^2,\]</span> и найти точку минимума аналитически уже не получается. Но наша целевая функция всё ещё непрерывная и дифференцируемая по набору весов. То есть оптимизационные методы первого порядка, такие как метод градиентного спуска, к нашим услугам.</p>
<p>Если вы хотите узнать подробнее, как ещё можно искать минимум, можно посмотреть на <a href="https://habrahabr.ru/post/318970/">публикацию</a> на Хабре.</p>
<p>Рассматривая далее целевую функцию как функцию от набора параметров, будем обозначать её <span class="math inline">\(J(\theta)\)</span>, разумея под <span class="math inline">\(\theta\)</span> все веса и смещения, а под <span class="math inline">\(\Theta\)</span> – множество всех возможных значений <span class="math inline">\(\theta\)</span>.</p>
<h3 id="метод-градиентного-спуска">Метод градиентного спуска</h3>
<p>Краткое описание метода: возьмём какую-нибудь точку <span class="math inline">\(\theta_0 \in \Theta\)</span> и пойдём в направлении наискорейшего спуска из этой точки <span class="math inline">\(\theta_{n+1} = \theta_n - \lambda_n \nabla J(\theta_n)\)</span>. Доказательство искать на <a href="https://en.wikipedia.org/wiki/Gradient_descent">Википедии</a>.</p>
<p>Для того, чтобы сделать один шаг градиентного спуска, нужно посчитать производные целевой функции по всем весам сети. Для среднеквадратичной ошибки в качестве целевой функции каждый элемент <span class="math inline">\(\nabla J(\theta)\)</span> будет выглядеть как</p>
<p><span class="math display">\[\frac{\partial J}{\partial w_{ij}^l} = \frac{1}{N} \sum_{k=1}^N \sum_{i=1}^{n_L}\frac{\partial}{\partial w_{ij}^l} \left( \hat y_{ki} - y_{ki} \right),\]</span></p>
<p>то есть сумма производных для всех входов. То есть для одого шага градиентного спуска нужно пройти по всем данным. Данных может быть ОЧЕНЬ много, так что это не вариант.</p>
<p><em>Стохастический градиентный спуск</em> предлагает корректировку: выбираем случайное подмножество наблюдений (как вариант, одно случайное наблюдение), считаем производную целевой функции для него и сдвигаемся в направлении антиградиента для этого конкретного наблюдения.</p>
<p>T.<span class="citation">(Powell <a href="#ref-powell2007approximate">2007</a>)</span>: если градиент целевой функции ограничен, минус градиент в среднем указывает в сторону минимума и градиент не равен нулю не в точке оптимума, то стохастический градиентный спуск сойдётся к оптимальному значению.</p>
<p>Заметим, что “градиент не равен нулю не в точке оптимума” – это очень сильное условие. Это означает, что никаких локальных минимумов/максимумов, никаких сёдел и уж тем более частично постоянных функций.</p>
<h3 id="алгоритм-обратного-распространения-ошибки">Алгоритм обратного распространения ошибки</h3>
<p>Самый важный вопрос: как считать-то этот градиент? Ответ: аналитически.</p>
<p>Пусть <span class="math inline">\(J(\theta)\)</span> – наша целевая функция, значение которой на множестве примеров <span class="math inline">\(\{x_i\}_ {i=1}^N\)</span> мы хотим минимизировать по множеству параметров <span class="math inline">\(\theta\)</span>. Тогда для каждого параметра <span class="math inline">\(w_ {ij}^l\)</span> нужно посчитать (см. уравнение )</p>
<p><span class="math display">\[\frac{\partial J}{\partial w_{ij}^l} = \frac{\partial J}{\partial a_i^l} \cdot \frac{\mathrm{d} a_i^l(x)}{\mathrm{d} x} a^{l-1}_ j.\]</span></p>
<p>Мы уже видим повторяющийся для всех весов нейрона <span class="math inline">\(i\)</span> на слое <span class="math inline">\(l\)</span> элемент <span class="math inline">\(\frac{\partial J}{\partial a^l_i}\)</span>, но из чего же состоит он сам? Из</p>
<p><span class="math display">\[\frac{\partial J}{\partial a^l_i} = \sum_{m=1}^{n_{l+1}} \frac{\partial J}{\partial a^{l+1}_ m}\cdot\frac{\mathrm{d} a^{l+1}_m}{\mathrm{d} x} w^{l+1}_{mj}.\]</span></p>
<p>То есть для того, чтобы посчитать производные на следующем слое, достаточно производные на предыдущем слое (уже неизбежно подсчитанные) умножить на известные константы.</p>
<p>Изложенное выше не включает в себя векторизацию, после которой формулы становятся красивее, но которая утомительнее в выписывании.</p>
<p>Подробное изложение с векторизацией см. в <span class="citation">(Nielsen <a href="#ref-Nielsen2015">2015</a>)</span>. Хороший пост про численные недостатки этого алгоритма есть <a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">у Андрея Карпаты</a>.</p>
<h2 id="примеры-неглубоких-сетей">Примеры неглубоких сетей</h2>
<h3 id="autoencoder">Autoencoder</h3>
<!-- ![схема автокодировщика](https://upload.wikimedia.org/wikipedia/commons/3/34/%D0%90%D0%B2%D1%82%D0%BE%D1%8D%D0%BD%D0%BA%D0%BE%D0%B4%D0%B5%D1%80.png)
 -->
<p>Пусть данные описываются векторами размера <span class="math inline">\(N&gt;&gt;1\)</span>. Мы хотим получить их представление размерности <span class="math inline">\(k\)</span>, для чего строим полносвязную нейронную сеть с одним внутренним слоем ширины <span class="math inline">\(k\)</span> и выходным слоем ширины <span class="math inline">\(N\)</span>. После чего обучаем сеть на множестве примеров, у которых правильный ответ для примера равен самому примеру. Тогда когда мы остановим обучение сети, у нас будет представление наших данных в пространстве меньшей размерности. Причём качество этого представления мы сможем измерить.</p>
<ul>
<li>как универсальный кодировщик эта идея не заходит, т.к. является репрезентативной только внутри поданной ей выборки, плохо обобщается</li>
<li>хорошо заходит в качестве очищения от шума (всё равно мы не знаем, что в данных было шумом)</li>
<li>использовалась для предобучения слоёв в глубинных сетях, но residual networks победили её всухую</li>
<li>говорят, что учит фичи лучше, чем PCA</li>
</ul>
<p><a href="https://blog.keras.io/building-autoencoders-in-keras.html">tutorial with lots of info</a> – хорошее введение с примерами приложений и бесполезностей</p>
<h3 id="word2vec">Word2Vec</h3>
<p><a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a> <a href="https://www.tensorflow.org/versions/r0.10/tutorials/word2vec/index.html">Tutorial with net scheme and more on embeddings themselves</a></p>
<h2 id="проблемы">Проблемы</h2>
<p>Нейросети требовательны к объёму памяти и скорости вычислений, не очень хорошо параллелятся и т.д. При увеличении количества слоёв появляются проблемы с “размазыванием” градиента и с точностью вычислений. Не все данные представимы конечным разумным числом входов (см. “Картинки с большим разрешением”, “текстовые последовательности”, считай требуемую память, умирай). Не гарантируется скорость сходимости, поэтому хочется задавать априорную информацию о структуре взаимоотношений между входами.</p>
<h2 id="решения">Решения</h2>
<h3 id="convolutional-neural-networks">Convolutional neural networks</h3>
<p><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">wiki</a> find proper literature and pictures</p>
<h3 id="local-vs.global-optimization">Local vs. global optimization</h3>
<p>what the hell does it mean and why did I write this?</p>
<h3 id="optional-recurrent-neural-networks">[optional] Recurrent neural networks</h3>
<p>??? whatever else</p>
<div id="refs" class="references">
<div id="ref-cybenko1989">
<p>Cybenko, George. 1989. «Approximation by superpositions of a sigmoidal function». <em>Mathematics of control, signals and systems</em> 2 (4). Springer: 303–14.</p>
</div>
<div id="ref-Nielsen2015">
<p>Nielsen, Michael A. 2015. <em>Neural Networks and Deep Learning</em>. Determination Press. <a href="http://neuralnetworksanddeeplearning.com" class="uri">http://neuralnetworksanddeeplearning.com</a>.</p>
</div>
<div id="ref-powell2007approximate">
<p>Powell, Warren B. 2007. «Approximate Dynamic Programming: Solving the curses of dimensionality». John Wiley &amp; Sons.</p>
</div>
<div id="ref-Vorontsov2007">
<p>Воронцов, Константин Вячеславович. 2007. «Лекции По Искусственным Нейронным Сетям», 102–12. <a href="http://www.ccas.ru/voron/download/NeuralNets.pdf" class="uri">http://www.ccas.ru/voron/download/NeuralNets.pdf</a>.</p>
</div>
<div id="ref-Vorontsov2011">
<p>———. 2011. <em>Математические Методы Обучения По Прецедентам (Теория Обучения Машин)</em>. <a href="http://www.machinelearning.ru/wiki/images/6/6d/voron-ml-1.pdf" class="uri">http://www.machinelearning.ru/wiki/images/6/6d/voron-ml-1.pdf</a>.</p>
</div>
<div id="ref-Gorban1998">
<p>Горбань, А.Н., В.Л. Дунин-Барковский, А.Н. Кирдин, и others. 1998. <em>Нейроинформатика</em>. Новосибирск: Наука. <a href="http://ict.edu.ru/ft/003873/neiro.pdf" class="uri">http://ict.edu.ru/ft/003873/neiro.pdf</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Слова “нейрон” и “перцептрон” не являются взаимозаменяемыми. Нейроны могут иметь иной вид (см. <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a>), перцептрон — конкретный подвид нейрона.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Тут важно заметить, что вообще-то от архитектуры сети тоже зависит то, насколько хорошо мы сможем приблизить функцию (вспоминаем ещё раз пример про исключающее или), но перебор ещё и по архитектуре мы пока строить не будем.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p><span class="math inline">\(n_1 = m\)</span>, то бишь первый слой сети размерности входных данных<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Время вспомнить про двойственность обозначений <span class="math inline">\(a_j^l\left(\cdot\right)\)</span><a href="#fnref4">↩</a></p></li>
</ol>
</div>
</body>
</html>
