---
title: |
    Ранжирующие бандиты
author: Ершов Васили
bibliography: sources.bib
link-citations: true
lang: russian
mainfont: "Times New Roman"
---
$$\DeclareMathOperator*{\argmax}{arg\,max}$$

#  Многорукие бандиты.

![Бандиты](img/bandit.png)

Рассмотрим следующую задачу: игрок приходит в казино, в котором стоит $n$ игровых автоматов (игровые автоматы (one slot machine) называют бандитами, отсюда и название),  у каждого из которых есть ручка, за которую можно «дернуть». Каждый «бандит» при дергании ручки дает какой-то выигрыш. Задача игрока — найти «лучшего» бандита и заработать как можно больше денег. 
Основная проблема игрока — он никогд не знает точно, какая же из ручек «оптимальна». Каждый раз, выберая ручку,  у него есть только информация о предыдущих испытания и две «базовые» стратегии — либо использовать тот автомат, от которого было больше всего пользы до этого — эксплуатирующая стратегия, либо некоторым образом пытаться использовать другие автоматы для того, чтобы понять, а не дадут ли они существенную пользу в будущем. Оптимальная стратегия игры — комбинация exploation и exploration (explotation-exploration tradeoff).

Существует несколько вариантов формализации задачи о многоруком бандите. Нас в первую очередь будут интересовать многорукие бандиты со случайным выигрышем (stochastic multi-armed bandits, smab). В  данной постановки выигрыш является случайной величиной, зависящей от «ручки». Другая популярная постановка задачи — adversarial bandits, в которых казино, на основе действий игрока, может менять выигрыши ручек. Первая постановка задачи близка к теории вероятностей и статистики, а вторая к теории игр.  

Рассмотрим формализацию задачи. У нас имеется множество ручек $A=\\\{a_i\\\}, 1 \leq i \leq k$. Задан дискретный поток времени $1, 2…$, в момент времени $t$ игрок выбирает ручку $a(t)$ и получает некоторый выигрыш $g_t = g_t(a(t)) = g_{a, t}$ (gain, reward, часто обозначают за $r$, но для того, чтобы не «путать» с regret будем обозначать буквой $g$). Ручки $a(t)$ выбираются игроком на основе некоторой стратегии (policy) — «функции» от истории. 

Цель игрока заключается в максимизации discounted cummulative reward за $T$ шагов:
$$G(T) = E\left( \sum\limits_{t=1}^{T} \lambda^{t}g_{t}\right), 0 < \lambda \leq 1 $$

В обычной постановке задачи $\lambda = 1$. В случае, когда $\lambda < 1$ ее можно трактовать как вероятность того, что после момента времени $t$ игра продолжится.

Достаточно часто от максимизации кумулятивного выигрыша переходя к минимизации потерь (regret):
$$\mu = \max\limits_{i = 1, … k}Eg(a_i)$$
$$R(T) = T\mu - E\sum\limits_{t = 1}^{T} g_{t},$$
где $g_{a_i}$ — выигрыш $i$-ой ручки, $\mu$ — «лучшая ручка», а параметр $\lambda$ мы считаем равным 1.
Или вводят случайные $r_t = r_t(a_t)$ (regret ручки) и сразу минимизируют 
$$R(T) = E\sum \limits_{i=1}^{T}r_{t}$$ 

Из определения потерь/выиграша видно, что для бейзлайн алгоритм (берущего случайную ручку) потери будет расти линейно.

Для такой постановки разработана обширная теоретическая база, доказываются upper/lower bounds на потери/выигрыш, предложены алгоритм, близкие или достигающие оптимальной границе (в зависимости от более точной формализации задачи).

### Варианты решения SMAB 

Для решения SMAB существует несколько основных алгоритмов. Обычно предполагается, что $0 \leq g(a) \leq 1$ (т.е. выигрыш ограничен).

В случае $\lambda < 1$ и байесовской постановки задачи (предполагаем, что для ручки — из параметрического семейства, для каждой ручки есть априорное распределение) можно показать (Gittins), что существует оптимальная стратегия игры (максимизирующая G(T), при этом не ассимптотически). Для каждой ручки надо вычислять так называемый dynamic allocation index (gittins index) и брать ручку с максимальным значением индекса. Сам алгоритм является некоторой вариацией динамического программирования и является очень трудоемким, из-за чего на практике его применять не получается. 

В связи с тем, что «оптимальный» алгоритм на практике не применим, используют другую технику — Upper confidence bounds (UCB).
Основная идея в том, что для каждой «ручки» $a$ в момент времени  $t$ есть некоторая оценка $\hat{\mu}(a)$ среднего выигрыша этой ручки. Пусть она построена по  $n_a(t)$ наблюдениям. Тогда предлагается построить для каждой ручки доверительный интервал (верхнюю границу) уровня $\delta(t)$:
$$ q(a) = \hat{\mu}(a) + \phi(\alpha, n_a(t), t)$$ 
И выберем ручку с максимальным значением q(a). $\alpha$ является параметром алгоритма, «регулирующем» exploration/explotation tradeoff.
Типичный пример:
$$q(a) = \hat{\mu}(a) + \sqrt{\frac{\alpha \log t}{n_a(t)}}$$

Видимо лучшей UCB на данный момент является KL-UCB, которая, в частности, для выигрышей, распределенных по бернулли, достигает нижней границы на потери (т.е. асимптотически опимальна). 
Для бернулли оптимально:
 $$\liminf \limits_{T \rightarrow \infty} \frac{ER(T)}{\log T} \geq C(\mu_1, …, \mu_k)$$ 

Помимо UCB для решения SMAB, в байесовской постановке SMAB, используют Thompson sampling.


Замечание: мы рассматривали ситуация, когда выигрыш стационарен. 

# Ранжирующие бандиты

### Постановка задачи

Для начала рассмотрим неформальное описание решаемой задачи. 

Есть некоторый поисковик/рекомендательная система/etc. В нее приходит пользователь и задает запрос $q$. Для каждого запроса есть множество документов $\mathbb{D} = \mathbb{D}(q)$. Требуется выбрать $k$ документов, показ которых удовлетворит пользователя (т.е. он найдет то, что искал). (документы, которые подходят под запрос называют релевантными).  

Цель поисковой системы — правильно отвечать на запросы пользователей. При этом, для конкретного пользователя система не знает, остался ли он доволен и имеет только неявный feedback о том, на какие документы и «как долго» кликал пользователь.

Требуется разработать систему, которая будет показывать «оптимальные» документы для пользователя. Различные формализация понятия «оптимально» приводят к различным алгоритмам.

Как видно, все очень похоже на то, что мы видели до этого:
1. Есть некоторый feedback о том, удачный ли мы результат показали пользователю
2. Можем жадно показывать документы с максимальным числом кликов на текущий момент
3. Можем пытаться искать новые документы, про кликабельность которых мы ничего не знаем


Мы рассмотрим 2 принципиально разных подхода к решению данной задачи.

### Сведения задачи к MAB/SMAB

Основные особенности первого семейства подходов, которые мы будем рассматривать:
1. Рассматриваем алгоритмы в рамках одного запроса. Для разных запросов — отдельные «экземпляры» алгоритма
2. В качестве ручки — список из $k$ документов, который показываем пользователю.
3. Сводим задачу к обычным MAB

Первый алгоритм, который мы рассмотрим — ranking bandtis by Radlinski. 
Решается следующая задача: есть множество документов $\mathbb{D}$ мощности $n$, есть некоторая «популяция» пользователей $U$. От ранжирующей системы требуется выдавать $k$ таких документов, что пользователь кликнет хотя бы на один из них (Цель — минимизация adandonment rate, т.е. кол-ва пользователей, которые ушли ни с чем).  

Одна из задач, которая решается рассмотренным далее алгоритмом — разнообразие выдачи. Существуют запросы, которые для разных пользователей означают разное. Например, на запрос «ягуар» один пользователь может иметь в виду животное, а другой автомобиль. Соответсвенно на выдачи надо показывать, с одной стороны, как можно более разнообразный набор документов, а с другой такой, что по нему будут «хорошо» кликать.

Рассмотрим следующую модель поведения пользователя:
1. Открываем страницу с выдачей
2. Смотрим на документы (точнее на snippets) по-порядку, начиная с первого.
3. Как только находим интересующий документ — кликаем и «уходим».

В момент времени $t$ к нам приходит пользователь $u_t$. Предполагаем, что пользователь характеризуется набором вероятностей $\\\{p_{t}(d)\\\}$, где $p_{t}(d)$ — вероятность пользователя кликнуть на документ $d$, если он до него дошел. 
Чуть позже нам потребуется «идеальный» кейс — ситуация, когда есть множество релевантных для пользователя $u_t$ документов $A = \\\{d_1, …, d_m\\\}$ и если пользователь его видит, то кликает с вероятностью 1.

Введем выигрыш: 
$$g_t(u_t) = \begin{cases}
    1, & \text{пользователь кликнул на какой-нибудь документ}.  \\
    0, & \text{иначе}.
  \end{cases}
$$

Пусть $a_t$ — список показанных в момент времени $t$ документов.

Будем максимизировать 
$$G(T) = E\sum\limits_{t=1}^{T} g_t(a_t, u_t)$$

Идея алгоритма  очень проста. 

Для каждой позиции $1…k$ в запросе вводится экземпляр бандитского алгоритма $MAB_{i}$. 

Для каждого пользователя будем действовать по следующей схеме:

Пусть $a_t[i], i = 1…k$ — показанный для $i$-ой позиции документ  в момент времени $t$

Для каждой позиции $i=1…k$ 
1. Выбераем с помощью $MAB_i$ документ.
2. Если документ встречается на $a_t[1], …, a_t[i-1]$, то заменяем его на случайный еще не выбранный документ
3. Показываем выбранный документ на позиции i (записываем в a_t[i])

После того, как по алгоритму выше сгенерирован $a$, показываем $a$ пользователю.
Пользователь кликает на какой-то документ, не умаляя общности будем считать, что это последний.

Тогда если документ на $k$ позиции не случайный, то $MAB_{k}$ получает выигрыш 1, иначе 0.
Все остальные документы получают выигрыш 0.


При некоторых предположения на алгоритм $MAB_i$ можно получть оценку на качество алгоритма за $T$ шагов:
$$G(T) = (1 - \frac{1}{e})OPT - O(k\sqrt{T\log n})$$

Здесь OPT — является оптимальным выигрышем для задачи в следующем смысле:

1. Рассматриваем идеальную ситуацию, когда $p_{ti}$ равны 0 или 1.
2. Множества релевантных документов для пользователей известны
3. Решаем задачу в оффлайн — по $u_1, …, u_{T}$ ищем такой набор документов, что $G(T)$ будет максимально (замечание: набор документов один на всех пользователей, а не лучший для каждого разумеется.)

Таким образом  для каждого документа есть набор пользователей, которые на него кликают. Итого есть $S_1, …, S_n$ множеств. Требуется выбрать из этих множеств $k$ так, чтобы $\cup S_i$ было максимальным (накрыть максимум пользователей)

Эта задачи $NP$-трудная.  OPT — оптимальное решение.

(1 - 1/e)OPT — жадное решение,  лучше которого сделать «нельзя» (или докажем какой-нибудь факт про  NP)


Вывод: в такой постановке алгоритм делате что-то полезное (на бесконечности «средний» выигрыш стремится к оптимальном, при этом скорость $\sqrt{T}$)

Еще замечание: 
1. «Бесплатно» получаем оптимальных бандитов и на префиксах из первых $l$ документов
2. Для доказательства оценки требуется выполнение некоторых условий на выигрыш для базового MAB-алгоритма. При этом для доказательства используется adversarial постановка задачи, про которую мы практически ничего не упоминали
3. В таком случае граница качества — worst case, т.е. что бы не происходило, будем не слишком плохие результаты показывать. В частности, выигрыш может быть нестационарным.
4. В статье использовали Exp3 алгоритм для базовых бандитов (он adversarial).
5.  Если интересы пользователей не меняются существенно со временем, то можно и нужно использовать алгоритмы на основе UCB, т.к. они, скорее всего, дадут «лучший» результат.

 
### Cascading bandits

Вводим больше ограничений в модель пользователя из прошлой задачи. 

Предполагаем, что  модель поведения пользователя на выдаче из $k$ документов следующая: 

1. Как и раньше, смотрим последовательно. 
2. Документы «притягивают» пользователя независимо друг от друга.
3. Для каждого документа задана вероятноть $p(d)$ (стационарная, не зависит от предыдущих просмотренных документов).
4. Кликает пользователь только на 1 документ

Из формулировки модели следует, что

1. Вероятность посмотреть на $d_i$ равна 
 $$\prod\limits_{j=1}^{i-1} \left(1 - p(d_j)\right)$$
2. Вероятность кликнуть хотя бы на 1 документ: $$1 - \prod\limits_{j=1}^{k} \left(1 - p(d_j)\right)$$
 
 Цель — выдать такой список документов, что вероятность того, что пользователь кликнет на какой-либо документ максимальна. 

За счет независимости выигрыш зависит только от $p(d_i)$ — «кликабельности» документа:
$$ \mu_i = p(a_t[i])$$
$$c_i = Ber(\mu_i)$$
$$g(a_t, u_t) = 1 - \prod\limits_{i=1}^{k}  (1 - c_i)$$
$$Eg(a_t, u_t) = 1 - \prod\limits_{i=1}^{k}(1 - \mu_i)$$

Как обычно определим потери как разницу между лучшим набором документов и тем, что показал пользователь.
$$R(T) = E\left(\sum\limits_{t=1}^{T} \left(g^{\*}(u_t) - g(a_t, u_t)\right)\right)$$
где $g^{\*}$ — алгоритм, показывающий всегда оптимальный набор документов (можно показать, что этот набор — документы с наибольшей вероятностью клика. Для тех, кому не очевидно — упражнение по теор. веру =)).

Алгоритм для такой постановки задачи предлагается следующий:

1. Выберем какой-нибудь UCB-like алгоритм. Рекомендуют KL-UCB, как наиболее точную.
2. Для каждого документа будем хранить кол-во раз, которое на него кликнули и кол-во раз, которое на него посмотрели
3. UCB будем вычислять для CTR — сколько раз кликнули при скольки просмотрах
4. На каждом шаге алгоритма — выбираем для показа $k$ документов с максимальными UCB
5. Для обновления UCB берем все документы до позиции последнего клика. Если пользователь не кликнул ни на один документ, то берем все документы. После этого все документы до позиции последнего клика считаем как просмотренные. На тот, который кликнул считаем и как кликнутый. Теперь с помощью UCB мы можем обновить статистики для этих документов (т.е. считаем, что «ручки», соответсвующие этим документам были сыграны)
 
Про данный алгоритм известно следующее:
1. $\liminf \frac{R(T)}{\log T} \geq C(k, n, p) $
2. Для KL-UCB и UCB1 $R(T) = O(n\log T)$, константы уменьшаются при увеличении $k$. Здесь $n$ в $O$ только для того, чтобы показать, что верхняя граница линейна по кол-во документов, из которых выбирают.

Стоит отметить, что авторы статьи получают результат, не зависящий от порядка, в котором показываются документы. Есть две стратегии того, как можно показывать топ $k$ документов по UCB:
1. По возрастанию UCB (первый — с самой низкой UCB)
2. По убыванию

С одной стороны, первый вариант «интуитивно» должен давать в будущем более точные оценки (позволяет «быстрее» узнать «полезную инфомрацию» про документы), с другой вариант 1 может раздрожать пользователей. На симулированных экспериментах вариант 1 был лучше, но почему авторы объяснить не смогли.

Замечание: есть обобщение данной модели на DCM модель, в которой пользователь может кликать на несколько документов:
![DCM model](img/dcm.png)


## Dueling bandits

Второй вариант алгоритма основан на иной идее. Будем считать, что у нас есть некоторое нормированное векторное пространство $\mathbb{W}$, задающее ранжирующие алгоритмы. Т.е. $w \in \mathbb{W}$ является функцией от запроса и документа $w = w(d, q)$, выдающая значение «релевантности» документа запросу, по которой затем сортируются документы и берутся первые $k$.

 Будем считать, что умеем сравнивать 2 алгоритма: 
$$P(w_1 > w_2) = \frac{1}{2} + \varepsilon(w_1, w_2)$$

А также показывать пользователю одновременно выдачу, сформированную с помощью $w_1$ и $w_2$. 

Определим потери алгоритма за время $T$ как
$$ R(T) = \sum\limits_{t=1}^{T} \varepsilon(w^{\*}, w_1) + \varepsilon(w^{\*}, w_2), $$
где $w^{*}$ — лучший алгоритм ранжирования(которого мы не знаем)

Тогда для оптимизации таких потерь предлагается использовать следующий алгоритм:
![Dueling bandint gradient descent](img/dbgd.png)

При некоторых дополнительных предположения можно показать, что приведенный выше алгоритм действительно будет оптимизировать введенную нами функцию потерь:

1. Существует  диффиренцируемая строго выпуклая utility-functon на $\mathbb{W}$ :
 $$v : \mathbb{W} \rightarrow \mathbb{R}$$
2. Существует $\sigma$ такая, что 
$$P(w_1 > w_2) = \sigma(v(w_1) - v(w_2))$$
$$ \varepsilon(w_1, w_2) = \sigma(v(w_1) - v(w_2)) - \frac{1}{2}$$
Здесь $\sigma$ — link function. Типичный пример link function:
$$ \sigma(x) = \frac{1}{1 + e^{-x}}$$
3. $\sigma, v, \sigma^{\'}$  — липшицевы.
4. $\mathbb{W}$ содержим 0,  компактное, выпуклое, ограничено

Тогда доказывается следующий результат про $\Delta_{T} = R(T)$:
![Dueling bandint gradient descent](img/dbgd_regret.png)

Здесь $R$ — радиус шара, в котором содержится $\mathbb{W}$, $L$ — константа липшица для $\varepsilon$ (равна $L_{v}L_{\sigma}$), $d$ — размерность $\mathbb{W}$


Замечание: в постановке считаем, что можем сравнить одновременно два алгоритма. Один из способов это сделать — TDI:

1. Смешиваем выдачи от двух разных ранкеров. 
2. Первый ранкер выбираем случайно
3. Каждый «ранкер» поочередно выбирает лучший документ из тех, что еще доступны
4. Добавляем документ в выдачу (на последнюю позицию), а выбранный документ записываем в «комманду» выбравшего его «ранкера»
5. Ранкер, по команде которого больше раз кликнули побеждает.


## Минусы рассмотренных подходов

В них практически никак не используется offline-алгоритмы, умеющие «хорошо» ранжировать документы (по сути некоторый prior на то, что есть хорошо и что есть плохо.). Кроме того, эти алгоритмы никак не умеют работать с контекстом — признаками, описывающими документы (и запросы). 

Кратко про то, как обучается формула для ранжирования. Для этого используется supervised learning подход с релевантностью, размеченной людьми и некоторым кол-вом признаков (фактор, features), на основе которых требуется предсказать релевантность документа запросу. 

Классическая схема обучения с учителем:

1. Собираем набор данных  (запрос, документ, релеватность). Для каждого запроса есть несколько документов.
2. Придумываем набор признаков, которые можем посчитать.
3. Обучаем GBDT под оптимизацию ранжирующей метрики (а точнее ее «гладкой» аппроксимации, но для нас это не суть важно).
4. Profit, afaik, state of the art качество.


Таким образом, для рассмотренных до этого бандита, обрабатывающего запрос, приходит не множество документов, а множество пар (doc, rel), где rel — некоторое число, отсортировав по которому есть шансы получить «хорошее» значение ранжирующей метрки.

### Offtop про идеи объединения подходов
«Идеи» подходов к объединению двух типов ранжирования:
1. С помощью supervised learning выдаем распределения релевантностей документов по факторам. Используем это распределение внутри бандитских алгоритмов
2. Формулу для ранжирования можно использовать в dueling bandits для начального подбора модели
3. Для некоторых простых классификаторов есть алгоритмы, позволяющие обучаться в online, а также confidence bounds на ошибку классификатора, которые также можно пытаться использовать
  
  



